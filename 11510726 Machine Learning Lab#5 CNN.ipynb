{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Lab 4 CNN\n",
    "### 11510726 毛煦冰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-ec7491b4d2f1>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/apple/Desktop/机器学习作业/ML_Lab_NN/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /Users/apple/Desktop/机器学习作业/ML_Lab_NN/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /Users/apple/Desktop/机器学习作业/ML_Lab_NN/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/apple/Desktop/机器学习作业/ML_Lab_NN/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/Users/apple/Desktop/机器学习作业/ML_Lab_NN/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 200\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "dropout = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, weights, biases, dropout):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-431b342d9791>:7: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 61275.7031, Training Accuracy= 0.164\n",
      "Step 10, Minibatch Loss= 24297.4375, Training Accuracy= 0.266\n",
      "Step 20, Minibatch Loss= 11658.5859, Training Accuracy= 0.484\n",
      "Step 30, Minibatch Loss= 6517.2231, Training Accuracy= 0.617\n",
      "Step 40, Minibatch Loss= 5123.2793, Training Accuracy= 0.742\n",
      "Step 50, Minibatch Loss= 5052.8047, Training Accuracy= 0.750\n",
      "Step 60, Minibatch Loss= 3443.0840, Training Accuracy= 0.781\n",
      "Step 70, Minibatch Loss= 2497.9575, Training Accuracy= 0.828\n",
      "Step 80, Minibatch Loss= 2957.2646, Training Accuracy= 0.844\n",
      "Step 90, Minibatch Loss= 1546.5166, Training Accuracy= 0.914\n",
      "Step 100, Minibatch Loss= 1935.3900, Training Accuracy= 0.859\n",
      "Step 110, Minibatch Loss= 2125.1665, Training Accuracy= 0.883\n",
      "Step 120, Minibatch Loss= 2219.0205, Training Accuracy= 0.891\n",
      "Step 130, Minibatch Loss= 1081.8143, Training Accuracy= 0.891\n",
      "Step 140, Minibatch Loss= 2032.3396, Training Accuracy= 0.891\n",
      "Step 150, Minibatch Loss= 812.3031, Training Accuracy= 0.914\n",
      "Step 160, Minibatch Loss= 1034.2910, Training Accuracy= 0.898\n",
      "Step 170, Minibatch Loss= 568.5620, Training Accuracy= 0.953\n",
      "Step 180, Minibatch Loss= 1608.5171, Training Accuracy= 0.922\n",
      "Step 190, Minibatch Loss= 1425.7738, Training Accuracy= 0.906\n",
      "Step 200, Minibatch Loss= 880.9351, Training Accuracy= 0.945\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.94140625\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images[:256],\n",
    "                                      Y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.FileWriter(\"/Users/apple/Desktop/机器学习作业/ML_Lab_NN/data/log/\", sess.graph)\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:https://colab.research.google.com\n",
    "  PreLab NN and github**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](graph_large_attrs_key.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=\"40%\">\n",
    "<img src=\"2.png\" width=\"40%\">\n",
    "<img src=\"3.png\" width=\"40%\">\n",
    "<img src=\"4.png\" width=\"40%\">\n",
    "<img src=\"5.png\" width=\"40%\">\n",
    "<img src=\"6.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEYCAYAAAAu1uNdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAHGpJREFUeJzt3XnUbFV55/Hvj0GMAyKNOOsF0V60A6AgohKvgIoRHBo1Kg7Y2mocYtTWbjUroHGMs8EBNHIVkMSoIAICXuAyKgqCcUiII9oIzqLYKtPuP/Z+eYu69Q733neo2u/3s1atOnXqnFP7qXOqntrn7L0rpRQkSerNZstdAEmSFoMJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkglOktQlE5wkqUsmOElSl0xwkqQumeAkSV0ywUmSumSCkyR1yQQnSeqSCU6S1CUTnCSpSyY4SVKXTHCSpC6Z4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkglOktQlE5wkqUsmOElSl0xwkqQumeAkSV0ywUmSumSCkyR1yQQnSeqSCU6S1CUTnCSpSyY4SVKXTHCSpC6Z4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkglOktQlE5wkqUsmOElSl0xwkqQumeAkSV0ywUmSumSCkyR1yQQnSeqSCU6S1CUTnCSpSyY4SVKXTHCSpC6Z4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJrgJk2R1ksuTrGu3281zvUOSjMX+NgZjWCgthjcNPF6T5KVJHjdiucOG5q1KsmZpSjqzHmIYV2NxkGqDHV1KWd1uV89znUOYx/5ewi8uY5iFMWySU0spJy/Tay+UHmJYdia4CZdkz/br+/wkz23zXpvk7CQXJtktyYOBXYEzkjyrLb9FW3Zdu1+T5HDg1FQfSnJmkpOT3N4YjGESYmhWJ3l+K8vHkqwFnjUQ5xuTnAu8cmDeAUnOSXJBkv3bvC8lOTzJpVPzllAPMSy/Uoq3CboBq4HLgXXAUcBpwNZAgLXALYBbtWV3Ao5t0+uALUZNt/s1wNPb9IHAa9v0Y6emjcEYxjyGdcBVwPPb7cHAR9pyrwMOA+5MrR0BPKOVdbOpOFrMZ7Tn/xPYHrgrcPxifJ57iWFcb1ugSXR0KeVvAZJcBZzY5m8H3AE4IMnBwI1AGbH+4LwMTF/c7ncGnpbkMdQPzJcWsOxTjMEYFspgDGsG5u8IXDJQnr2AewL/NjDv0dRYd6YmdYDtkwT4eSnlZ2272yxCuQf1EMPYMcFNvkuAJ5dSfp9ky1LKdUleDOwG3Av4SFvuOmBz4HrgauDOSf4A3GlgWze2+8uAT5RS3gWQZEtjMIYJiWHQD4BHtund2v3lwP2H5v0C+AbwmFLKDS32kmSm5L2Ueohh2XgNbvIdCpyY5Czgn9u8rwDnAM8dWO5k4IQkBwFHAp+nnu74+YhtngisatdNzqSeWlpMxmAMC66UciGwVZIzgPu0eVcCF7frVw9v824E3k29rngW8N6lKuNceohhOaWdp5UkqSvW4CRJXTLBSZK6ZIKTJHVpRSW4DA2Js5Hb+B8LVR5Jiy/Jrkme16Yn8vM7CTEkeUySc1vn/3cn2XyG5Ta4/EmemGTbDV1vRSW4BTKWB5ek0Uopl5ZS/qk9nMjP77jHkGQ74PXA/qWU1dQWtf9zhsU3pvxPBExw85Hk60k+0e53HZh3XJKLk+zR5p3X7le14YceD9y//UJ5VJKPpw5jdFaWaNy9Vgs9IcnnkpyX5OAkZ6QOg3TXVpbzknxwYPnPJ/lCa6q9bZKd2nA+ZyV53VKU2xiMYZFiOCTTQ1od1sr4uVbO85PcZurMTZIXMP353SXJSa3cn1rqcvcWA/A4amf137fH7wGeNI/v0HVJ3pnkyy02MjT8W5J7APsDxyZ5dZKXtOXPSvLA2Qq1IhMctUPqC4AXA89p8+5O/cXxeGp/oPWUUk4EvtF+oawD7lZKeQSwT+uHslRSSnkCcArw4FLKvsAV1OF4HlVKeTiwdZJ7t+VvWUp5LHAENe7VwJGllEcCb13Ccg8yBmNYNKWUA6kx7Tsw70imP7+/B37Ryv2Xy1LIOUxYDHcGfjL1oJTyR+qQYTcz+B1aSvlim3088DDgkCSj1vkRcCpwcCnlHcATgEe2uC8ZXn7QSk1w32074Apgm4F515RSrgCG/zZkvREASinXAR9PcgzwpiztyOnfbPc/GZq+N/Dp1EFvHw7cpT03dRBcSh1T8FPAA5IcS/1ltByMwRgWwqiROqbiGPx833ylUr4LfKOV+xWLV7x56SGGK5k+RkhyS+qINzfNmmXdS0opN1BHaNmemYd/m3Io8KEkR7blZ7RSE9yoN3CnJLdOchfgt23eLdv9/QeWLwCpF1CPK6U8kzpm3x6LWN5hZYbpuwEntF935zMd2y4D998DriulvJI6OsUbF7eoMzIGY1gIV1NrDzD9OZ3tC3Lq87sV8J5SysHA/knuuKilnF0PMXwBeHaSW7fHrwBOYJbv0AG7tO/TewI/Y3r4t+2YHv5tang4gEtLKYdQz6IdMluhHIty2o+Bj1F/lb64zTu5nUO+cGC5ryQ5AXgX8Oa2Y35LHQduud0IvCrJE4fmX5fkVOrBdhDw+CQvBW4FHLPEZZyLMYyHSYnhDODVqdfNr5/H8j9O8hngDcAH2rWe71O/WJfLxMdQSvlZkrdS/yLpRmpN/x3AHef4DgV4CnVosaNKKde2mtnngfOYHv7tNOCDSf4V2CvJDsBW3HwIufU4VFeT5Lx2vaErSVYD+02NVD6JjGE89BCDxks7Bb5fKWU+iX2DrdRTlJKkzlmDkyR1yRqcpImV5C5JvpbkjwN9p96TOqLG+9rjPVtfvXOTvGdo/YOS/Hg5yj5Qhi5jGAcrLsFt6I4YdWAl2SzJMamdvNe21j6j1j2wdUj8UpJXtXm3Su1Iuy61M+dWG1j++w2U56jZDvxZtrFlK9M1SXYaem7WD0uS3ZJ8I8kPRzz3/tRuE93H0JZ9dmrH6HVJ7trmvbJdVJ/P+vu3ddcluXJEg46p5Q4ZWO7XaYMTLIQOYvgVtZ/Yl9vrPBC4dSllb+AWqQ03Lqf2Vd2b+k/Xgy36nkxtYLacuothXKy4BMeIHZHqRak9489O8q4kU33hRh1YuwLXtk7eRwEHz/BaX6d2YHwotbXZ7ah9hS5sza+/wob3HbqslPLQVh6onSlHHvhJntIS8LokH00y1RT5eurQN58esf25PizfBR4C/N/BmalNlFetlBhaMnhEKWXf1mn1ivZjZZeh5WY8tkopp7Z1VwM/AtaOeq1Sypq2zH7U4/Hr8ynjSoihlPLHUsqvB2btNVCGtcBDSilXtX6vUI+bG1pcjwO+yPS/jy+LTmMYCysuwc2wI94B/AbYtyWttcBxSTaf4cC6gum+HNsAv2y1gk8CpNbudi+l/KiUckOpFzpvoB6E36M2b71p3Q0s/2DnyT8B35nhwH8VtQ/JY9sXy5HUoW62LtVPh7c9/GGZIabflenheAa9AvjHlRID8Bhg81b7+cfU7iLPBz4+tNyMx9ZAmXcEflpKuWZUeQe29efAOWXhLpz3EMOwbZjux3o1cPuBMj4A2K6U8u026zmMX/cM6COGsbDiEtywJLsB/0Gt0Z2Wes57T+Bo4MCB5QYPrF9Q/0b+34G/Aj5bSrkE+EGSI4CflFIuGlj3sdSRUn4HfAfYM8m3gN2BCzaizI9P8k1qL/5fDpcvyZ2Y7pf02SQfB/47dTDU2QY6vdmHZbaYhsqzLbWz+3dWUAx3BG5R6tBW/49aa3xEKeXMgW3O69hqcR0/j/LetNwC6SGGYb8Btm7TW7fHU/v3cGBqRP59gAtKKdcuYlk2Vg8xjIUVn+CAfainuV5C7eD9QeppqkupwxWtd2ABjwauLqXsDBwG/K82/8NtmfdNbbz9sn0N00PpPAc4rZRyX+Bk4JkbWuBSyomllPtRa5IHjCjfw6gdJZ8BvL+99v0GYxo2y4dlvZhGeHl7/ZUUw9XA2W36TOoAAZ8cWmbOY6s5EDhxtvImCXXIrHM2oIxz6SGGYV9ieuzG/YAvp15rPwZ4dSnlqvbc/aiXDU4F7ptN/ButBdZDDGPBBFeHwblFu586lQh10OWvzXBghXotD2ptbup63duoX5RvBEhyW2AN8LyBU2IzrTu/wt68UcpvqUPYjCrfYExT5+cfD1w8w6Zn+rDcLKYZ7EAdaPfjwD5Jntp7DNSa9wPa9K7UIYP+auC1XzYihpsdWwCtpnptKWXwVPWo8u4BfK3UMfsWysTHkNrYaC31uuFpwJbAH5OcC9xYSvkKdaSMPYC3p17L3auU8v5Syj6llP2Bby1n5/UeY0iy53KV5WZKKSvqRj141gK/pg6RsyfwQuoX3FrqkDFrgZe35Z9OHS5mXbvtRR3i7NPt8TnAvagNHt7S1nkf8CjgtdTGDlPr7kA9v35ae/xFYNsNLP8TqL+6zwY+Sm3gMly+u1KH8dkWOImaZD8DvAnYrG3nU9RBdc8HnjD0Gue1+1Ex3b29P79p96sG1lsFHLMSYmjLvrOV99PUU33Dr70HsxxbbZkXAi8deLxeedv0W4EDFuHzMPExePM2082O3tSm4cC51C/QQh1jb20p5TfLWrBNkOT11CT+kVLKdUn2A75XSvnBMhdt3jqJYeKPrR5i0MrkKcrqb4B7UGtjp1MbG1y9rCXaRKWUN1P/L+qMJFM11eXuK7NBeoiBPo6tHmLQCmQNTpLUJWtwkqQujc2YYUvhpJNOGovq6gEHHDDbv9vOaiFi2JTXXwgrIYblLt9c5rMPJimGcS/rTIxhcVmDkyR1aUXV4CT1Y7C28IY3vKEAHHrooWNVg5hLbzGMGxPcBLv44pn6O8+53k2nFJb7w9RrDIPlg+Uv41x6iEEa5ilKSVKXrMFJmnhTtctxbvAwlx5iGDfW4CRJXbIG15EHPehBG7zOcHPx5f7F2GsMk/arfFJj6K3RxqTGMC5McNJGGPwSGm6MMQmGE9QkxiDNxQQnqUs9XNPqIYbl5DU4SVKXrMHN4YgjjpjxuRe+8IVLWJINtzF9zMat71OPMYxb+eajhxi08pjgpE00/OU+LmOebogeYphJD402eohhOXiKUpLUJWtwK8TGNL+H8bq43WsM49bNYS6THEMPjTZ6iGGpWIOTJHXJGpy0wHroY9ZDDJIJbg7j1lJy0jsYQx8xaLL10GijhxgWmwluheqx+f081xn7GMbpr4Dmo4cY1CcTnLTIBr/gJ7X5fQ8xzGS40cYkNtjoIYbFYIKbYD30XeohBknjyVaUkqQuWYMT0O/f1Mxl3GMYt/LNx6TGMFWuqQYbMHnXD3uIYSGZ4KQl1EPz+x5i0MpggutID188PcSgyTaqQc241jpn0kMMC8EEp/VsTPP7tt7YnBbpNYZx6+Ywlx5i0OSykYkkqUvW4KRl1EM3iR5imEkPjTZ6iGFjmeA61kPn3MWI4aKLLho5f/fdd1+IzUsaEyY4SZpDD402eohhQ5ngpDEy20DUUzXPca9pOpi2xoUJboXoofl9DzFo8vVwTauHGObDBCdNiHGvuUnjxm4CkqQuragaXO8XVDU/k1IT6uGUUQ8xzKSHRhs9xDCbFZXgNK2Hvks9xCBp8ZjgJGkT9dBoo4cYhpngtEEGO0lPyqk+SSuTjUwkSV1KKV620M0NnqLYWMt9aqOHGDTZemi0MekxWIOTJHXJa3CStAiGG21M4hmBSY/BBKf1TNpBPEoPMUjaNJ6ilCR1yUYmkrQEBgcimNRGG5MWgzU4SVKXrMFJ0hKb1EYbgyYhBmtwkqQumeAkSV3yFKUkqUvW4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkgluiSXZN8nZSc5JcnyS/7LA2z9vlueemGTbNn1IkgdtxPZXJ7k+yfbt8R5JSpJVG1vmjZFkTZKdkuyf5HEbsb77YQFs6n4Ysb3VSS5Psq7dLkmyxSzL7timd03ywE19/YUwIobbjVhmXZItkhyWZL/lKOdseogBYOSBo8WRZDvg74ADSim/S3If4BZLWIQnAt8EflVKWbMJ27kUeALwEeBJwEWbXrSNU0o5dUPXcT8svI3ZD7M4upTyt1C/RGdZbjVwHvB9YFfq99nXFrAcm+KmGCbYxMdgDW5pPY560PwOoJTyn8D2Sc5P8uUkz4SbfhW/L8l5SQ5NslWSL05tJMkZSbZM8sy23vlJdhl8oalfVwPT9wD2B45N8uqpX13tF9hxrSZzXHu8Osnnkny+bfs2Q3GcCezbpu8LfAvYptWIvpzkde11X5bkJUlu1cpw2xFlfGdb5wVt3i4j3o/15g1s45Akz0+yKsmZST6d5OIkd2vPH5VkbZKPJTnM/TBW+2HeUmuIZye5KMmzk9wCOAR4V5J3AS8AXp3k2FQfauU4Ocnt5/FeLpoM1OYzQ8Ju5b1fm/6bJActUfHmZVJjMMEtrTsDVw7N+3vgYGBv4GVJtmzz15VSHg78RSnlT8DPk9wj9RTUj4Ebgb9u6x0MvHm2Fy6l/Ag4FTi4lPKOgaeeBHy7lPLn1C/IgwbWORA4hekv0SnXAn9M8hDg39u8PwGrSykPAR6V5M+Aw4EDqDWMd0wllCHHAw8DDmlfWqPej5neo2G3B54KvBs4KMmewJ9KKfsBlw0s535Y33Lsh5k8qyXdowbmnVNKeQTwEOAFpZRrgTXAq0oprwKObLEd3GL9USllnxb7i6Y2Mst7udBGxTCbY4GntenHAicvTrE2yMTHYIJbWlcCdxmad/tSyg9LKdcBPwC2b/O/2e7/0O4/Q/3SO6hN3wG4vJRyXSnlh8DwOfLBf7LNLGW6F9OndS4Cdhp6/SuAbUasdwrwYeCzA69xSpKzgZ2B7Uv9N91jgN1LKSfDTbWedUnu1Na7pJRyA3B5i33U+zHTezTs26WUGwfKvAPwb+25SweWcz+Mx36YydGllNWllOcOzHtQkrXAGdTa6mx2Bp7WahqvB7Zt8+d6LxfSejEkmW3/nw88pP1wurKU8sdFLt98THwMJrildQrwzKlTREl2ov4CX9V+De8I/KwtO/xX66dQT209Cjgd+DmwKvUU2Srg6qHlrwbunHq9aepL7Dpg86Hlvg9MNXLYHfjeiNcfdVCfAlwMfLU9fjvw9vYr+7s1vNwaeD7wqSTPASil7Ns+NFe19XZJsjlwzxb7b0a8H6PmjTJc5h8A92+PHzBUdvfD8u+HDfGaFsN+TL/Hg+/j4PRlwCdafA8HXjdDuZZKkmzF9HuwnvYj5CvAO4B/XqqCbYCJjMFGJkuolPLzJH8PnNR+Cf0K+D/AJ6kfzg+UUq4b9SOplPKHJL8GrmunykhyOHAu9TTZS4ZWORL4PPUi/M/bvNOADyb514HljgeOSXIOtWbzduqpqrliuQZ4XisH1NMRhyf5NvXUGdTTdW+j/ur+QpJTSyk/HdrUU4D3AkeVUq5N8ncj3o9R8+YqIqWUC5O8KMkZwE+A/2jz3Q9jsB820PHA56g1wF+3eeuAt7RToMcCa9o1oL8G3p/kzLbce4HfbsRrLpQ11P0/V0OcY6kxPX2Ry7Mx1jCBMaQmXWnptVNI+5VSrl/E19iilHJ9kv9NvS5z3GK91qRyP4yHJP8NeHEp5aXLXZaNNW4xWINT7/4pyQ7UX/BPXu7CrGDuh1kk2Rv4B+A5y12WjTWOMViDkyR1yUYmS6hdoP9pa712+izLDY4i8IkRz9/Ut2oRyrimNboY9dyeSS5Icm6S98yyjQNT+0p9KcmrRjy/LOVvz0/0PmjPT/x+0MJpx/Q+81x2dTaiH+JiW6wYTHBL74utddej51huqonuszf1BZMs1H6+HNinlLI3tWP0TC2qvk5tIPFQ4PEZMczPhljA8k+Z5H0A/ewHLYxVwLySwxhbxSLE4AG79B7Zfnm/AiDJbkk+2aaPSbJ7W+7pbbkZWyMleUaS97Xp57flz00bky/J15McA7wmQ6NytOd3SnJ66ggRcw7JU0q5aqBvy/XADaPKX0r5USnlhtZs+AZq68JlL/+Aid0H0NV+6FKSzZJ8tL0fX0gdqebL7bZfW2Zdkrcm+WqS580yb882//wkz23zHtYen5XkL6mjuDwrtZUqSf6urXNm2tikqSPIrAWetVJiAKCU4m2JbsBWwK2pjXtOBh7Q5r8ZOAL4h/b4NsCWbdkvAXcY2s464JnA+9vj7YATqX17tgVOaPN/Cdy6Ta8BntSmL2z3/wLcvU0fB9ytLbfTHHE8ADh54PHNyj8w/7HAkSPWX7by97IPJn0/9Hyjjkrzlja9GbV5/dbtdsHAe79bOx7PmWXeaW29AGupY6aeC2w3sP3VwJva4/sDR7Tpndvx8GDgI23e64DDVkIMpRRbUS6lUvtNTfWdOgm4H3WEhw9TO8Pesy13TVvlutR+Ufdmug/VlNcCD2/TOwK7AGcNLXNZKeX3A4+HR+X4r8DRqX2ZtgHuOlcMqaPgH04dimnKzcrfltuR2jn3gBk2tSzl72EftLJP9H7o3H2ACwBKKTcmKaWU3wIkuWFguW+W2pfwxlnm7UL90QH1B8gd2nZ/MbD9wdfeGVid6fEir6Tu10va44uBvVZIDJ6iXEq5+SC3D2N6tIq3AS8H3tiW27rdbw7sAfxwxOaeQ+0Y/GfUL7Wvlnq9aDV1lA1Y/5TUcJPZy4Cnt3UexPRoGDOVfwvqkE+vLtMjYIwq/22pv+CfN/TFuKzlHyjblInbB61ME78fOncZdczMqeuWmyXZuh1TgyPYjGrCPjzvEuBx7b3drZRyBVDS/t6pbX94FJfTB/bhs6n7dWoQ8N1WUAwmuCW2d+oI6xcAPyl1hIcnUscy/ABwTZJHAU9N8hXq2G6fK6X8ZMS2LqUOifMJ6sgOJ6eORH8WdVSO+Xg98LHUER9OAW41x/JPoX7Zv72dH99rhvK/FNihbXtdav+ncSg/TP4+gD72Q89OpA7Pdg5wEvUHx+nAF4E3bOC2DgVObPtjavir1wKfb/OeQq1RPyzJv5RSvg5c1fb3WcBzSykXAlu161v3WUEx2A9OktQna3CSpC6Z4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkglOktQlE5wkqUsmOElSl0xwkqQumeAkSV0ywUmSumSCkyR1yQQnSeqSCU6S1CUTnCSpSyY4SVKXTHCSpC6Z4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkglOktQlE5wkqUsmOElSl0xwkqQumeAkSV0ywUmSumSCkyR1yQQnSeqSCU6S1CUTnCSpSyY4SVKXTHCSpC6Z4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkglOktQlE5wkqUsmOElSl0xwkqQumeAkSV0ywUmSumSCkyR1yQQnSeqSCU6S1CUTnCSpSyY4SVKXTHCSpC6Z4CRJXTLBSZK6ZIKTJHXJBCdJ6pIJTpLUJROcJKlLJjhJUpdMcJKkLpngJEldMsFJkrpkgpMkdckEJ0nqkglOktQlE5wkqUsmOElSl0xwkqQumeAkSV0ywUmSuvT/Aa/N/Mce2H7eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "%matplotlib inline\n",
    "\n",
    "NumConvMax = 8\n",
    "NumFcMax = 20\n",
    "White = 1.\n",
    "Light = 0.7\n",
    "Medium = 0.5\n",
    "Dark = 0.3\n",
    "Black = 0.\n",
    "\n",
    "\n",
    "def add_layer(patches, colors, size=24, num=5,\n",
    "              top_left=[0, 0],\n",
    "              loc_diff=[3, -3],\n",
    "              ):\n",
    "    # add a rectangle\n",
    "    top_left = np.array(top_left)\n",
    "    loc_diff = np.array(loc_diff)\n",
    "    loc_start = top_left - np.array([0, size])\n",
    "    for ind in range(num):\n",
    "        patches.append(Rectangle(loc_start + ind * loc_diff, size, size))\n",
    "        if ind % 2:\n",
    "            colors.append(Medium)\n",
    "        else:\n",
    "            colors.append(Light)\n",
    "\n",
    "\n",
    "def add_mapping(patches, colors, start_ratio, patch_size, ind_bgn,\n",
    "                top_left_list, loc_diff_list, num_show_list, size_list):\n",
    "\n",
    "    start_loc = top_left_list[ind_bgn] \\\n",
    "        + (num_show_list[ind_bgn] - 1) * np.array(loc_diff_list[ind_bgn]) \\\n",
    "        + np.array([start_ratio[0] * size_list[ind_bgn],\n",
    "                    -start_ratio[1] * size_list[ind_bgn]])\n",
    "\n",
    "    end_loc = top_left_list[ind_bgn + 1] \\\n",
    "        + (num_show_list[ind_bgn + 1] - 1) \\\n",
    "        * np.array(loc_diff_list[ind_bgn + 1]) \\\n",
    "        + np.array([(start_ratio[0] + .5 * patch_size / size_list[ind_bgn]) *\n",
    "                    size_list[ind_bgn + 1],\n",
    "                    -(start_ratio[1] - .5 * patch_size / size_list[ind_bgn]) *\n",
    "                    size_list[ind_bgn + 1]])\n",
    "\n",
    "    patches.append(Rectangle(start_loc, patch_size, patch_size))\n",
    "    colors.append(Dark)\n",
    "    patches.append(Line2D([start_loc[0], end_loc[0]],\n",
    "                          [start_loc[1], end_loc[1]]))\n",
    "    colors.append(Black)\n",
    "    patches.append(Line2D([start_loc[0] + patch_size, end_loc[0]],\n",
    "                          [start_loc[1], end_loc[1]]))\n",
    "    colors.append(Black)\n",
    "    patches.append(Line2D([start_loc[0], end_loc[0]],\n",
    "                          [start_loc[1] + patch_size, end_loc[1]]))\n",
    "    colors.append(Black)\n",
    "    patches.append(Line2D([start_loc[0] + patch_size, end_loc[0]],\n",
    "                          [start_loc[1] + patch_size, end_loc[1]]))\n",
    "    colors.append(Black)\n",
    "\n",
    "\n",
    "def label(xy, text, xy_off=[0, 4]):\n",
    "    plt.text(xy[0] + xy_off[0], xy[1] + xy_off[1], text,\n",
    "             family='sans-serif', size=8)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    fc_unit_size = 2\n",
    "    layer_width = 40\n",
    "\n",
    "    patches = []\n",
    "    colors = []\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "    size_list = [28, 14, 14, 7, 7]\n",
    "    num_list = [1, 32, 32, 64, 64]\n",
    "    x_diff_list = [0, layer_width, layer_width, layer_width, layer_width]\n",
    "    text_list = ['Inputs'] + ['Feature\\nmaps'] * (len(size_list) - 1)\n",
    "    loc_diff_list = [[3, -3]] * len(size_list)\n",
    "\n",
    "    num_show_list = list(map(min, num_list, [NumConvMax] * len(num_list)))\n",
    "    top_left_list = np.c_[np.cumsum(x_diff_list), np.zeros(len(x_diff_list))]\n",
    "\n",
    "    for ind in range(len(size_list)):\n",
    "        add_layer(patches, colors, size=size_list[ind],\n",
    "                  num=num_show_list[ind],\n",
    "                  top_left=top_left_list[ind], loc_diff=loc_diff_list[ind])\n",
    "        label(top_left_list[ind], text_list[ind] + '\\n{}@{}x{}'.format(\n",
    "            num_list[ind], size_list[ind], size_list[ind]))\n",
    "\n",
    "\n",
    "    start_ratio_list = [[0.4, 0.5], [0.4, 0.8], [0.4, 0.5], [0.4, 0.8]]\n",
    "    patch_size_list = [5, 2, 5, 2]\n",
    "    ind_bgn_list = range(len(patch_size_list))\n",
    "    text_list = ['Convolution', 'Max-pooling', 'Convolution', 'Max-pooling']\n",
    "\n",
    "    for ind in range(len(patch_size_list)):\n",
    "        add_mapping(patches, colors, start_ratio_list[ind],\n",
    "                    patch_size_list[ind], ind,\n",
    "                    top_left_list, loc_diff_list, num_show_list, size_list)\n",
    "        label(top_left_list[ind], text_list[ind] + '\\n{}x{} kernel'.format(\n",
    "            patch_size_list[ind], patch_size_list[ind]), xy_off=[26, -65])\n",
    "\n",
    "\n",
    "    size_list = [fc_unit_size, fc_unit_size, fc_unit_size]\n",
    "    num_list = [1024, 1024, 1]\n",
    "    num_show_list = list(map(min, num_list, [NumFcMax] * len(num_list)))\n",
    "    x_diff_list = [sum(x_diff_list) + layer_width, layer_width, layer_width]\n",
    "    top_left_list = np.c_[np.cumsum(x_diff_list), np.zeros(len(x_diff_list))]\n",
    "    loc_diff_list = [[fc_unit_size, -fc_unit_size]] * len(top_left_list)\n",
    "    text_list = ['Hidden\\nunits'] * (len(size_list) - 1) + ['Outputs']\n",
    "\n",
    "    for ind in range(len(size_list)):\n",
    "        add_layer(patches, colors, size=size_list[ind], num=num_show_list[ind],\n",
    "                  top_left=top_left_list[ind], loc_diff=loc_diff_list[ind])\n",
    "        label(top_left_list[ind], text_list[ind] + '\\n{}'.format(\n",
    "            num_list[ind]))\n",
    "\n",
    "    text_list = ['Flatten\\n', 'Fully\\nconnected', 'Fully\\nconnected']\n",
    "\n",
    "    for ind in range(len(size_list)):\n",
    "        label(top_left_list[ind], text_list[ind], xy_off=[-10, -65])\n",
    "\n",
    "    colors += [0, 1]\n",
    "    collection = PatchCollection(patches, cmap=plt.cm.gray)\n",
    "    collection.set_array(np.array(colors))\n",
    "    ax.add_collection(collection)\n",
    "    plt.tight_layout()\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    fig.set_size_inches(8, 2.5)\n",
    "\n",
    "    fig_dir = './'\n",
    "    fig_ext = '.png'\n",
    "    fig.savefig(os.path.join(fig_dir, 'convnet_fig' + fig_ext),\n",
    "bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference：https://blog.csdn.net/u013719780/article/details/53197462"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN's Accuracy is 0.9371 and CNN's Accuracy is 0.94140625 which is better than NN. The layer structure of NN is one-dimensional, and the layer structure of CNN is high-dimensional. NN deals with \"linear\" data, and CNN is intuitively more suitable for \"structural\" data. Fully connection will lead to some unreasonable result when dealing with figure. In the other hand, the point in figure almost only related to nearby point so i think thats why CNN is better than NN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
